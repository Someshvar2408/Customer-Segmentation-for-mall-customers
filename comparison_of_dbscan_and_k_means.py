# -*- coding: utf-8 -*-
"""Comparison of DBSCAN and K-means.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SIiXoi7lLqeBUh7W0ufOYmj2nv9Hg9s5

#COMPARSION OF K-MEANS AND DBSCAN
"""

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, DBSCAN
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, silhouette_samples
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.decomposition import PCA

data = pd.read_csv("mall_customers.csv")
data

features = ["Age", "Gender", "Annual Income (k$)", "Spending Score (1-100)"]
X = data[features]

data.info()

corr_matrix = X.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Heatmap of Feature Correlations')
plt.show()

"""## Data Pre-Processing"""

# Remove rows where features have z-scores greater than a threshold (e.g., 3)
z_scores = np.abs(stats.zscore(X[["Age", "Annual Income (k$)", "Spending Score (1-100)"]]))
X = X[(z_scores < 3).all(axis=1)]

# Check for missing values and handle them
X.fillna({'Age': X['Age'].mean()}, inplace=True)
X.fillna({"Annual Income (k$)":X["Annual Income (k$)"].mean()}, inplace=True)
X.fillna({"Spending Score (1-100)":X["Spending Score (1-100)"].mean()}, inplace=True)

# # Convert Age and Income to numeric
# X["Age"] = pd.to_numeric(X["Age"], errors='coerce')
# X["Annual Income (k$)"] = pd.to_numeric(X["Annual Income (k$)"], errors='coerce')

# One-hot encode "Gender"
X = pd.get_dummies(X, columns=["Gender"])

# Standardize features for K-means and DBSCAN
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data into training and testing sets (80%/20%)
X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)

"""### K-means Clustering"""

# Determine the optimal number of clusters using silhouette score
silhouette_scores = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_train)
    silhouette_scores.append(silhouette_score(X_train, kmeans.labels_))

silhouette_scores

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Elbow method
wcss = []
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_train)
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(10, 6))
plt.plot(range(1, 10), wcss, marker='o', linestyle='--')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

kmeans = KMeans(n_clusters=2, random_state=42, n_init=10 )
kmeans.fit(X_train)

train_labels_kmeans = kmeans.predict(X_train)

# Silhouette score for K-means
silhouette_avg_kmeans = silhouette_score(X_train, train_labels_kmeans)
print("K-means Silhouette score (training data):", silhouette_avg_kmeans)

# Plotting the clusters
plt.scatter(X_train[:, 0], X_train[:, 1], c=kmeans.labels_, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', marker='X')
plt.title('KMeans Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

pca = PCA()
pca.fit(X_scaled)

# Calculate the explained variance ratio
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance = explained_variance_ratio.cumsum()

# Plot the explained variance
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(explained_variance_ratio) + 1), cumulative_variance, marker='o', linestyle='--')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance by Number of Principal Components')
plt.grid()
plt.show()

data['Cluster'] = kmeans.predict(X_scaled)

# Apply PCA for 2D visualization
pca = PCA(n_components=4)
pca_components = pca.fit_transform(X_scaled)
data['PCA1'] = pca_components[:, 0]
data['PCA2'] = pca_components[:, 1]

# Plot the clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(data=data, x='PCA1', y='PCA2', hue='Cluster', palette='viridis', s=100)
plt.title('K-Means Clustering with PCA')
plt.show()

"""## DBSCAN Clustering"""

dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan.fit(X_train)

train_labels_dbscan = dbscan.labels_

# Silhouette score for DBSCAN (only if there are more than one clusters)
if len(set(train_labels_dbscan)) > 1:
    silhouette_avg_dbscan = silhouette_score(X_train, train_labels_dbscan)
    print("DBSCAN Silhouette score (training data):", silhouette_avg_dbscan)
else:
    silhouette_avg_dbscan = -1  # Not applicable if DBSCAN fails to find clusters
    print("DBSCAN did not find more than one cluster")

"""## Visualization for both K-means and DBSCAN"""

# PCA for dimensionality reduction
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train)

plt.figure(figsize=(10, 5))
# Plot for DBSCAN Clustering
plt.subplot(1, 2, 1)
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=train_labels_dbscan)
plt.title("DBSCAN Clustering - PCA Visualization")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")

# Plot for K-means Clustering
plt.subplot(1, 2, 2)
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=train_labels_kmeans)
plt.title("K-means Clustering - PCA Visualization")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")

plt.tight_layout()
plt.show()

# ----- Silhouette Plot for K-means -----
silhouette_values_kmeans = silhouette_samples(X_train, train_labels_kmeans)
plt.scatter(X_train[:, 0], silhouette_values_kmeans, c=train_labels_kmeans)
plt.title("Silhouette Plot for K-means Clustering")
plt.show()

# ----- Silhouette Plot for DBSCAN -----
if len(set(train_labels_dbscan)) > 1:
    silhouette_values_dbscan = silhouette_samples(X_train, train_labels_dbscan)
    plt.scatter(X_train[:, 0], silhouette_values_dbscan, c=train_labels_dbscan)
    plt.title("Silhouette Plot for DBSCAN Clustering")
    plt.show()

# ----- Comparison of Algorithms -----
print("Comparison of Clustering Methods:")
print(f"K-means Silhouette Score: {silhouette_avg_kmeans}")
if silhouette_avg_dbscan != -1:
    print(f"DBSCAN Silhouette Score: {silhouette_avg_dbscan}")
else:
    print("DBSCAN did not find more than one cluster")